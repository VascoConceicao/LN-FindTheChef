{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1b252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config (run before anything else)\n",
    "LOG_RESULTS = True\n",
    "INPUT_FILE_PATH = \"data/train.csv\"\n",
    "N_SPLITS = 5\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5511f",
   "metadata": {},
   "source": [
    "## Weak baseline\n",
    "\n",
    "The weak baseline establishes a foundational classification model using TF-IDF (Term Frequency–Inverse Document Frequency) vectorization on the raw description field and a Support Vector Classifier (SVC) with a linear kernel.\n",
    "\n",
    "- Only the description field is used as input.\n",
    "- The raw text is directly transformed with TfidfVectorizer, which converts words into numerical features based on their relative frequency across documents.\n",
    "- No additional preprocessing (such as stopword removal, lowercasing, or cleaning) is applied, keeping this baseline deliberately simple.\n",
    "\n",
    "This setup provides the simplest possible benchmark for the classification task. It helps establish how much predictive power comes purely from recipe descriptions, serving as a lower reference point against which richer models can be compared.\n",
    "\n",
    "### Evaluation: K-Fold Cross-Validation\n",
    "\n",
    "Performance is estimated using Stratified K-Fold Cross-Validation:\n",
    "\n",
    "- The dataset is partitioned into K folds of (almost) equal size.\n",
    "- Each fold is used once as the validation set, while the remaining K-1 folds form the training set.\n",
    "- Class distributions (chef_id) are preserved across folds to ensure fairness.\n",
    "- Results are averaged over all folds, providing a more robust and reliable accuracy estimate than a single 80/20 split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0461faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 'data/train.csv'...\n",
      "\n",
      "==================================================\n",
      "## Chef ID Distribution Check\n",
      "\n",
      "=== Original Dataset === (Size: 2999)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       806       26.88\n",
      "5060       534       17.81\n",
      "3288       451       15.04\n",
      "8688       432       14.40\n",
      "1533       404       13.47\n",
      "6357       372       12.40\n",
      "\n",
      "=== Fold 1 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.83\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        86       14.33\n",
      "1533        81       13.50\n",
      "6357        75       12.50\n",
      "\n",
      "=== Fold 2 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       162       27.00\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        86       14.33\n",
      "1533        81       13.50\n",
      "6357        74       12.33\n",
      "\n",
      "=== Fold 3 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.83\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        87       14.50\n",
      "1533        81       13.50\n",
      "6357        74       12.33\n",
      "\n",
      "=== Fold 4 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.83\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        87       14.50\n",
      "1533        81       13.50\n",
      "6357        74       12.33\n",
      "\n",
      "=== Fold 5 === (Size: 599)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.88\n",
      "5060       106       17.70\n",
      "3288        91       15.19\n",
      "8688        86       14.36\n",
      "1533        80       13.36\n",
      "6357        75       12.52\n",
      "==================================================\n",
      "\n",
      "Folds saved to 'data/weak_baseline/folds_not_preprocessed.csv' (Total: 2999 rows)\n"
     ]
    }
   ],
   "source": [
    "from utils import create_folds\n",
    "\n",
    "create_folds(\n",
    "    input_data=\"data/train.csv\",\n",
    "    output_file=\"data/weak_baseline/folds_not_preprocessed.csv\",\n",
    "    text_columns=[\"description\"],\n",
    "    n_splits=N_SPLITS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518f6b0",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccbdd968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model: SVC\n",
      "Cross-Validation (using data/weak_baseline/folds_not_preprocessed.csv)\n",
      "  Fold 1: 0.7117\n",
      "  Fold 2: 0.7000\n",
      "  Fold 3: 0.7550\n",
      "  Fold 4: 0.7183\n",
      "  Fold 5: 0.7195\n",
      "Mean accuracy: 0.7209  |  Std: 0.0206\n",
      "Total runtime: 4.84 seconds\n",
      "==================================================\n",
      "➕ Added new results for 'Weak baseline (without pre-processing): Linear SVC + TF-IDF on raw description'.\n",
      "✅ Results saved to results/log.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vasco/Documents/Code/find-the-chef/utils.py:138: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([new_row])[cols]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model: SVC\n",
      "Cross-Validation (using data/weak_baseline/folds_preprocessed.csv)\n",
      "  Fold 1: 0.6767\n",
      "  Fold 2: 0.6617\n",
      "  Fold 3: 0.6767\n",
      "  Fold 4: 0.6883\n",
      "  Fold 5: 0.6745\n",
      "Mean accuracy: 0.6756  |  Std: 0.0095\n",
      "Total runtime: 3.35 seconds\n",
      "==================================================\n",
      "➕ Added new results for 'Weak baseline (with pre-processing): Linear SVC + TF-IDF on raw description'.\n",
      "✅ Results saved to results/log.xlsx\n"
     ]
    }
   ],
   "source": [
    "from utils import run_kfold_experiment\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "_ = run_kfold_experiment(\n",
    "    folds_file=\"data/weak_baseline/folds_not_preprocessed.csv\",\n",
    "    model_cls=SVC,\n",
    "    model_kwargs={\"kernel\": \"linear\"},\n",
    "    model_desc=\"Weak baseline (without pre-processing): Linear SVC + TF-IDF on raw description\",\n",
    "    random_seed=RANDOM_SEED,\n",
    "    log_results=LOG_RESULTS\n",
    ")\n",
    "\n",
    "_ = run_kfold_experiment(\n",
    "    folds_file=\"data/weak_baseline/folds_preprocessed.csv\",\n",
    "    model_cls=SVC,\n",
    "    model_kwargs={\"kernel\": \"linear\"},\n",
    "    model_desc=\"Weak baseline (with pre-processing): Linear SVC + TF-IDF on raw description\",\n",
    "    random_seed=RANDOM_SEED,\n",
    "    log_results=LOG_RESULTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364e8c9",
   "metadata": {},
   "source": [
    "## Strong baseline\n",
    "\n",
    "The strong baseline continues to use TF-IDF vectorization in combination with a Linear Support Vector Classifier (SVC).\n",
    "Instead of relying only on the description field (as in the weak baseline), this approach leverages a richer textual representation by concatenating multiple fields into a single document:\n",
    "\n",
    "- recipe_name\n",
    "- description\n",
    "- tags\n",
    "- steps\n",
    "- ingredients\n",
    "- data (date field, cast to text)\n",
    "- n_ingredients (numeric field, cast to text)\n",
    "\n",
    "This combined text ensures that the classifier has access to broader contextual information about each recipe.\n",
    "\n",
    "Evaluation: K-Fold Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0508e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 'data/train.csv'...\n",
      "\n",
      "==================================================\n",
      "## Chef ID Distribution Check\n",
      "\n",
      "=== Original Dataset === (Size: 2999)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       806       26.88\n",
      "5060       534       17.81\n",
      "3288       451       15.04\n",
      "8688       432       14.40\n",
      "1533       404       13.47\n",
      "6357       372       12.40\n",
      "\n",
      "=== Fold 1 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.83\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        86       14.33\n",
      "1533        81       13.50\n",
      "6357        75       12.50\n",
      "\n",
      "=== Fold 2 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       162       27.00\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        86       14.33\n",
      "1533        81       13.50\n",
      "6357        74       12.33\n",
      "\n",
      "=== Fold 3 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.83\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        87       14.50\n",
      "1533        81       13.50\n",
      "6357        74       12.33\n",
      "\n",
      "=== Fold 4 === (Size: 600)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.83\n",
      "5060       107       17.83\n",
      "3288        90       15.00\n",
      "8688        87       14.50\n",
      "1533        81       13.50\n",
      "6357        74       12.33\n",
      "\n",
      "=== Fold 5 === (Size: 599)\n",
      "         Count  Percentage\n",
      "chef_id                   \n",
      "4470       161       26.88\n",
      "5060       106       17.70\n",
      "3288        91       15.19\n",
      "8688        86       14.36\n",
      "1533        80       13.36\n",
      "6357        75       12.52\n",
      "==================================================\n",
      "\n",
      "Folds saved to 'data/strong_baseline/folds_not_preprocessed.csv' (Total: 2999 rows)\n"
     ]
    }
   ],
   "source": [
    "from utils import create_folds\n",
    "\n",
    "create_folds(\n",
    "    input_data=\"data/train.csv\",\n",
    "    output_file=\"data/strong_baseline/folds_not_preprocessed.csv\",\n",
    "    text_columns=[\n",
    "        \"recipe_name\",\n",
    "        \"data\",\n",
    "        \"tags\",\n",
    "        \"steps\",\n",
    "        \"description\",\n",
    "        \"ingredients\",\n",
    "        \"n_ingredients\",\n",
    "    ],\n",
    "    n_splits=N_SPLITS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0523569c",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06269120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model: SVC\n",
      "Cross-Validation (using data/strong_baseline/folds_not_preprocessed.csv)\n",
      "  Fold 1: 0.8667\n",
      "  Fold 2: 0.8500\n",
      "  Fold 3: 0.8667\n",
      "  Fold 4: 0.8600\n",
      "  Fold 5: 0.8464\n",
      "Mean accuracy: 0.8579  |  Std: 0.0094\n",
      "Total runtime: 19.63 seconds\n",
      "==================================================\n",
      "➕ Added new results for 'Strong baseline (without pre-processing): Linear SVC + TF-IDF on all fields'.\n",
      "✅ Results saved to results/log.xlsx\n",
      "==================================================\n",
      "Model: SVC\n",
      "Cross-Validation (using data/strong_baseline/folds_preprocessed.csv)\n",
      "  Fold 1: 0.8533\n",
      "  Fold 2: 0.8533\n",
      "  Fold 3: 0.8483\n",
      "  Fold 4: 0.8433\n",
      "  Fold 5: 0.8447\n",
      "Mean accuracy: 0.8486  |  Std: 0.0047\n",
      "Total runtime: 16.34 seconds\n",
      "==================================================\n",
      "➕ Added new results for 'Strong baseline (with pre-processing): Linear SVC + TF-IDF on all fields'.\n",
      "✅ Results saved to results/log.xlsx\n"
     ]
    }
   ],
   "source": [
    "from utils import run_kfold_experiment\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "_ = run_kfold_experiment(\n",
    "    folds_file=\"data/strong_baseline/folds_not_preprocessed.csv\",\n",
    "    model_cls=SVC,\n",
    "    model_kwargs={\"kernel\": \"linear\"},\n",
    "    model_desc=\"Strong baseline (without pre-processing): Linear SVC + TF-IDF on all fields\",\n",
    "    random_seed=RANDOM_SEED,\n",
    "    log_results=LOG_RESULTS\n",
    ")\n",
    "\n",
    "_ = run_kfold_experiment(\n",
    "    folds_file=\"data/strong_baseline/folds_preprocessed.csv\",\n",
    "    model_cls=SVC,\n",
    "    model_kwargs={\"kernel\": \"linear\"},\n",
    "    model_desc=\"Strong baseline (with pre-processing): Linear SVC + TF-IDF on all fields\",\n",
    "    random_seed=RANDOM_SEED,\n",
    "    log_results=LOG_RESULTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc07d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
